---
- name: Install GPU enabled POD
  hosts: nodes

  tasks:
    - name: Do we have a CUDA enabled instance?
      shell: lspci | grep NVIDIA
      register: lsnv
      failed_when: "lsnv.rc != 0"

    - name: remove all old and new kernels
      shell: |
        for i in `rpm -q kernel`
        do
             rpm -e $i;
        done

    - name: Install kernel specific for CUDA
      yum:
        allow_downgrade: yes
        name:  kernel-3.10.0-693.el7.x86_64
        state: present

    - name: Install kernel-devel specific for CUDA
      yum:
        allow_downgrade: yes
        name:  kernel-devel-3.10.0-693.el7.x86_64
        state: present

    - name: recreate grub2 config
      shell: grub2-mkconfig -o /boot/grub2/grub.cfg


    - name: get current time
      command: /bin/date +%s
      register: before_reboot

    - name: reboot system
      shell: sleep 2; shutdown -r now
      async: 1
      poll: 0
      ignore_errors: true


    - name: waiting for server to come back
      local_action:
        module: wait_for
          host    = {{ inventory_hostname }}
          state   = started
          port    = 22
          delay   = 30
          timeout = 180

    - name: verify a reboot was actually initiated
      # machine should have started after it has been rebooted
      shell: (( `date +%s` - `awk -F . '{print $1}' /proc/uptime` > {{ before_reboot.stdout }} ))
      sudo: false

      #dkms
    - name: Install epel repo from a remote rpm
      yum:
        name:  https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
        state: present

    - name: Install NVIDIA repo from a remote rpm
      yum:
        name: https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-9.0.176-1.x86_64.rpm
        state: present

    - name: Install xorg-x11-drv-nvidia
      yum:
        name: xorg-x11-drv-nvidia
        state: present

    - name: Install xorg-x11-drv-nvidia-devel
      yum:
        name: xorg-x11-drv-nvidia-devel
        state: present


    - name: Install clinfo for compute capability fact
      yum:
        name: clinfo
        state: present

    - name: create facts for GPU
      shell: clinfo


    - name: Remove nouveau
      command: modprobe -r nouveau

    - name: Load nvidia modules
      command: nvidia-modprobe

    - name: Check for GPU support
      command: nvidia-smi
      register: nsmi
      failed_when: "nsmi.rc != 0"

    - name: get NVIDIA GPU name
      shell: nvidia-smi --query-gpu=gpu_name --format=csv,noheader --id=0 | sed -e 's/ /-/g'
      register: nvgpuname

    - name: set fact nvgpuname for future use
      set_fact: NV_GPU_NAME={{ nvgpuname.stdout }}


    - name: Add NVIDIA container repos
      yum_repository:
        name:          "{{ item.name }}"
        description:   NVIDIA container (docker) YUM repo
        file:          "{{ item.name }}"
        baseurl:       https://nvidia.github.io/{{ item.name }}/centos7/x86_64
        repo_gpgcheck: yes
        gpgcheck:      no
        gpgkey:        https://nvidia.github.io/{{ item.name }}/gpgkey
        sslverify:     yes
        sslcacert:     /etc/pki/tls/certs/ca-bundle.crt
      with_items:
        - { name: 'libnvidia-container' }
        - { name: 'nvidia-container-runtime' }

    - name: Import rpm keys
      rpm_key:
        state=present
        key={{ item }}
      with_items:
         - https://nvidia.github.io/nvidia-container-runtime/gpgkey
         - https://nvidia.github.io/libnvidia-container/gpgkey

    - name: Update repo cache for the new repo
      command: yum -q makecache -y --disablerepo=* --enablerepo={{ item }}
      with_items:
        - libnvidia-container
        - nvidia-container-runtime

    - name: Install nvidia-container-runtime
      yum:
        name:  nvidia-container-runtime-0:1.0.0-1.docker1.12.6.x86_64
        state: present
        update_cache: yes

    - name: Create systemd drop-in directory for docker
      file:
        path: /etc/systemd/system/docker.service.d
        state: directory
        owner: root
        group: root


    # docker segfaults when using /etc/docker/daemon.json
    # using docker drop-in ...'
    - name: systemd docker drop-in
      shell: |
        echo '[Service]
              ExecStart=
              ExecStart=/usr/bin/dockerd-current \
                       --add-runtime nvidia=/usr/bin/nvidia-container-runtime \
                       --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \
                       --default-runtime=docker-runc \
                       --authorization-plugin=rhel-push-plugin \
                       --exec-opt native.cgroupdriver=systemd \
                       --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \
                       $OPTIONS \
                       $DOCKER_STORAGE_OPTIONS \
                       $DOCKER_NETWORK_OPTIONS \
                       $ADD_REGISTRY \
                       $BLOCK_REGISTRY \
                       $INSECURE_REGISTRY\
                       $REGISTRIES' > /etc/systemd/system/docker.service.d/add-nvidia-runtime.conf

    - name: systemd reload
      command: systemctl daemon-reload

    - name: docker restart
      command: systemctl restart docker

    - name: docker verify nvidia-runtime
      shell: systemctl status docker | grep 'nvidia=/usr/bin/nvidia-container-runtime'
      register: nvrt
      failed_when: "nvrt.rc != 0"


    - name: get yaml processor
      shell: |
        wget https://github.com/mikefarah/yaml/releases/download/1.13.1/yaml_linux_amd64
        mv yaml_linux_amd64 yaml
        chmod +x yaml
      args:
        chdir: /tmp

    - name: add feature-gate to node-config.yaml
      shell: /tmp/yaml w -i /etc/origin/node/node-config.yaml kubeletArguments.feature-gates[0] "Accelerators=true"

    - name: atomic-openshift-node restart
      command: systemctl restart atomic-openshift-node

    - name: verify feature gate
      shell: journalctl -u atomic-openshift-node --since="5 minutes ago" | grep feature | grep 'Accelerators:true'
      register: featgate
      failed_when: "featgate.rc != 0"


    # hack, when using nvidia-smi, only these character devices visible
    # ls /dev/nv*
    # /dev/nvidia0  /dev/nvidiactl
    # after deviceQuery
    # ls /dev/nv*
    # /dev/nvidia0  /dev/nvidiactl  /dev/nvidia-uvm  /dev/nvidia-uvm-tools
    - name: Install xorg-x11-drv-nvidia
      yum:
        name: cuda-9-0
        state: present

    - name: Run deviceQuery to have all character devices
      shell: /usr/local/cuda-9.0/extras/demo_suite/deviceQuery



- name: create gpu pod on ocp master (1)
  hosts: masters

  tasks:
    # Assuming here we have only on GPUNOE later maybe a lot more ...
    # Later one has to iterate over the group but for now ...
    - name: get hostname
      shell: echo {{ hostvars[groups.GPUNODE | first]['NV_GPU_NAME'] }}
      register: nvgpuname

    - name: get internal hostname of GPUNODE
      shell: ssh {{ groups.nodes | first }} hostname
      register: nvgpunode

    - name: oc label the GPUNODE
      shell: oc label node {{ nvgpunode.stdout }} alpha.kubernetes.io/nvidia-gpu-name="{{nvgpuname.stdout}}" --overwrite

    - name: oc describe GPUNODE
      shell: oc describe node {{ nvgpunode.stdout }} | egrep -B1 'Name:|gpu:'

    - name: oc create pod
      shell: oc create -f gpu-pod.yaml
...
