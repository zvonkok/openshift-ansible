---
- name: get the correct kernel-devel version from running kernel
  shell: echo kernel-devel-`uname -r`
  register: kernel_version

- name: try to install the same kernel-devel version as running kernel
  yum:
    name: "{{ kernel_version.stdout }}"

- name: Install epel repo from a remote rpm
  yum:
    name:  https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
    state: present


- name: Install NVIDIA repo from a remote rpm
  yum:
    name: https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-9.1.85-1.x86_64.rpm
    state: present

- name: Install xorg-x11-drv-nvidia
  yum:
    name: "{{ item }}"
    state: present
  with_items:
    - xorg-x11-drv-nvidia
    - xorg-x11-drv-nvidia-devel


- name: add NVIDIA container repos
  yum_repository:
    name:          "{{ item.name }}"
    description:   NVIDIA container (docker) YUM repo
    file:          "{{ item.name }}"
    baseurl:       https://nvidia.github.io/{{ item.name }}/centos7/x86_64
    repo_gpgcheck: yes
    gpgcheck:      no
    gpgkey:        https://nvidia.github.io/{{ item.name }}/gpgkey
    sslverify:     yes
    sslcacert:     /etc/pki/tls/certs/ca-bundle.crt
  with_items:
    - { name: 'libnvidia-container' }
    - { name: 'nvidia-container-runtime' }

- name: import rpm keys
  rpm_key:
    state=present
    key={{ item }}
  with_items:
    - https://nvidia.github.io/nvidia-container-runtime/gpgkey
    - https://nvidia.github.io/libnvidia-container/gpgkey

- name: update repo cache for the new repo
  command: yum -q makecache -y --disablerepo=* --enablerepo={{ item }}
  with_items:
    - libnvidia-container
    - nvidia-container-runtime

- name: install nvidia-container-runtime-hook
  yum:
    name: nvidia-container-runtime-hook
    state: present
    update_cache: yes

- name: create docker hook
  blockinfile:
    path: /usr/libexec/oci/hooks.d/oci-nvidia-hook
    block: |
      #!/bin/bash
          /usr/bin/nvidia-container-runtime-hook $1
    create: yes

- name: create cri-o hook
  blockinfile:
    path: /usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
    block: |
      {
          "hook": "/usr/bin/nvidia-container-runtime-hook",
          "stage": [ "prestart" ]
      }
    create: yes


- name: make the hook executable
  file:
    path: /usr/libexec/oci/hooks.d/oci-nvidia-hook
    mode: 0755

- name: Remove nouveau
  command: modprobe -r nouveau

- name: Load nvidia modules
  command: nvidia-modprobe

- name: Load nvidia-uvm module
  command: nvidia-modprobe -u

- name: Check for GPU support
  command: nvidia-smi
  register: nsmi
  failed_when: "nsmi.rc != 0"

- name: set fact nvgpuname for future use
  set_fact: NV_GPU_NAME={{ "0xDEADBEEF" }}

- name: get NVIDIA GPU name
  shell: nvidia-smi --query-gpu=gpu_name --format=csv,noheader --id=0 | sed -e 's/ /-/g'
  register: nvgpuname

- name: change selinux context of nvidia files
  shell: |
    chcon -t container_file_t  /dev/nvidia*


- name: set fact nvgpuname for future use
  set_fact: NV_GPU_NAME={{ nvgpuname.stdout }}


- name: add feature-gate to node-config.yaml
  yedit:
    src: /etc/origin/node/node-config.yaml
    key: kubeletArguments.feature-gates
    value:
    - "DevicePlugins=true"

- name: atomic-openshift-node restart
  systemd:
    state: restarted
    name: atomic-openshift-node

- name: verify feature gate
  shell: journalctl -u atomic-openshift-node --since="5 minutes ago" | grep feature | grep 'DevicePlugins'
  register: featgate
  failed_when: "featgate.rc != 0"
