---
- block:
    - name: check for the right kernel version
      shell: test `uname -r` == "3.10.0-693.el7.x86_64"
      register: version_check
      ignore_errors: true


    - block:
        - name: remove all old and new kernels
          shell: |
                for i in `rpm -q kernel`
                do
                    rpm -e $i;
                done

        - name: Install kernel specific for CUDA
          yum:
            allow_downgrade: yes
            name:  kernel-3.10.0-693.el7.x86_64
            state: present

        - name: Install kernel-devel specific for CUDA
          yum:
            allow_downgrade: yes
            name:  kernel-devel-3.10.0-693.el7.x86_64
            state: present

        - name: recreate grub2 config
          shell: grub2-mkconfig -o /boot/grub2/grub.cfg

        - name: get current time
          command: /bin/date +%s
          register: before_reboot

        - name: reboot system
          shell: sleep 2; shutdown -r now
          async: 1
          poll: 0
          ignore_errors: true

        - name: waiting for server to come back
          local_action:
            module: wait_for
              host    = {{ inventory_hostname }}
              state   = started
              port    = 22
              delay   = 30
              timeout = 180

        - name: verify a reboot was actually initiated
          # machine should have started after it has been rebooted
          shell: (( `date +%s` - `awk -F . '{print $1}' /proc/uptime` > {{ before_reboot.stdout }} ))
          sudo: false
      when: "version_check.rc != 0"
  when: fixture_get_specific_kernel_version


- name: set fact nvgpuname for future use
  set_fact: NV_GPU_NAME={{ "0xDEADBEEF" }}

  #dkms
- name: Install epel repo from a remote rpm
  yum:
    name:  https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
    state: present

- name: Install NVIDIA repo from a remote rpm
  yum:
    name: https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-9.0.176-1.x86_64.rpm
    state: present

- name: Install xorg-x11-drv-nvidia
  yum:
    name: "{{ item }}"
    state: present
  with_items:
    - xorg-x11-drv-nvidia
    - xorg-x11-drv-nvidia-devel
    - clinfo

- name: create facts for GPU
  shell: clinfo

- name: Remove nouveau
  command: modprobe -r nouveau

- name: Load nvidia modules
  command: nvidia-modprobe

- name: Check for GPU support
  command: nvidia-smi
  register: nsmi
  failed_when: "nsmi.rc != 0"

- name: get NVIDIA GPU name
  shell: nvidia-smi --query-gpu=gpu_name --format=csv,noheader --id=0 | sed -e 's/ /-/g'
  register: nvgpuname

- name: set fact nvgpuname for future use
  set_fact: NV_GPU_NAME={{ nvgpuname.stdout }}


- name: add NVIDIA container repos
  yum_repository:
    name:          "{{ item.name }}"
    description:   NVIDIA container (docker) YUM repo
    file:          "{{ item.name }}"
    baseurl:       https://nvidia.github.io/{{ item.name }}/centos7/x86_64
    repo_gpgcheck: yes
    gpgcheck:      no
    gpgkey:        https://nvidia.github.io/{{ item.name }}/gpgkey
    sslverify:     yes
    sslcacert:     /etc/pki/tls/certs/ca-bundle.crt
  with_items:
    - { name: 'libnvidia-container' }
    - { name: 'nvidia-container-runtime' }

- name: import rpm keys
  rpm_key:
    state=present
    key={{ item }}
  with_items:
    - https://nvidia.github.io/nvidia-container-runtime/gpgkey
    - https://nvidia.github.io/libnvidia-container/gpgkey

- name: update repo cache for the new repo
  command: yum -q makecache -y --disablerepo=* --enablerepo={{ item }}
  with_items:
    - libnvidia-container
    - nvidia-container-runtime

- name: install nvidia-container-runtime
  yum:
    name:  nvidia-container-runtime-0:1.0.0-1.docker1.12.6.x86_64
    state: present
    update_cache: yes

- name: upload nvidia-container-runtime docker hook
  copy: src=nvidia-container-runtime dest=/usr/libexec/oci/hooks.d

- name: change selinux context of /dev/nvidia*
  shell: |
    chcon -t svirt_sandbox_file_t  /dev/nvidia*
    chcon -u system_u              /dev/nvidia*

- name: add feature-gate to node-config.yaml
  yedit:
    src: /etc/origin/node/node-config.yaml
    key: kubeletArguments.feature-gates
    value:
    - "Accelerators=true"

- name: atomic-openshift-node restart
  systemd:
    state: restarted
    name: atomic-openshift-node

- name: verify feature gate
  shell: journalctl -u atomic-openshift-node --since="5 minutes ago" | grep feature | grep 'Accelerators:true'
  register: featgate
  failed_when: "featgate.rc != 0"
  # hack, when using nvidia-smi, only these character devices visible
  # ls /dev/nv*
  # /dev/nvidia0  /dev/nvidiactl
  # after deviceQuery
  # ls /dev/nv*
  # /dev/nvidia0  /dev/nvidiactl  /dev/nvidia-uvm  /dev/nvidia-uvm-tools





- block:
    - name: Install cuda-9-0
      yum:
        name: cuda-9-0
        state: present

    - name: Run deviceQuery to have all character devices
      shell: /usr/local/cuda-9.0/extras/demo_suite/deviceQuery
  when: fixture_device_query
